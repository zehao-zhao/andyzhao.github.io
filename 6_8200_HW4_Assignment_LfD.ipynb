{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zehao-zhao/andyzhao.github.io/blob/main/6_8200_HW4_Assignment_LfD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcyVmjGrSE-C"
      },
      "source": [
        "# Spring 2023 6.8200 Computational Sensorimotor Learning Assignment 4\n",
        "\n",
        "In this assignment, we will work on learning from demonstrations. \n",
        "\n",
        "You will need to **answer the bolded questions** and **fill in the missing code snippets** (marked by **TODO**).\n",
        "\n",
        "There are **140** total points to be had in this PSET.  `ctrl-f` for \"pts\" to ensure you don't miss questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQNYcagOR0_D"
      },
      "outputs": [],
      "source": [
        "!pip install pybullet > /dev/null 2>&1\n",
        "!pip install git+https://github.com/taochenshh/easyrl.git > /dev/null 2>&1\n",
        "!pip install git+https://github.com/Improbable-AI/airobot.git > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyrsDfdwj_S1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gym\n",
        "import pprint\n",
        "import time\n",
        "import pybullet as p\n",
        "import pybullet_data as pd\n",
        "import pybullet_envs\n",
        "import airobot as ar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from typing import Any\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from matplotlib import pylab\n",
        "from dataclasses import dataclass\n",
        "from airobot import Robot\n",
        "from airobot.utils.common import quat2euler\n",
        "from airobot.utils.common import euler2quat\n",
        "from gym import spaces\n",
        "from gym.envs.registration import registry, register\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "from itertools import count\n",
        "from easyrl.agents.ppo_agent import PPOAgent\n",
        "from easyrl.utils.common import save_traj\n",
        "from easyrl.configs import cfg\n",
        "from easyrl.configs import set_config\n",
        "from easyrl.configs.command_line import cfg_from_cmd\n",
        "from easyrl.engine.ppo_engine import PPOEngine\n",
        "from easyrl.models.categorical_policy import CategoricalPolicy\n",
        "from easyrl.models.diag_gaussian_policy import DiagGaussianPolicy\n",
        "from easyrl.models.mlp import MLP\n",
        "from easyrl.models.value_net import ValueNet\n",
        "from easyrl.agents.base_agent import BaseAgent\n",
        "from easyrl.utils.torch_util import DictDataset\n",
        "from easyrl.utils.torch_util import load_state_dict\n",
        "from easyrl.utils.torch_util import load_torch_model\n",
        "from easyrl.runner.nstep_runner import EpisodicRunner\n",
        "from easyrl.utils.torch_util import save_model\n",
        "from easyrl.utils.torch_util import action_entropy\n",
        "from easyrl.utils.torch_util import action_from_dist\n",
        "from easyrl.utils.torch_util import action_log_prob\n",
        "from easyrl.utils.torch_util import clip_grad\n",
        "from easyrl.utils.common import set_random_seed\n",
        "from easyrl.utils.gym_util import make_vec_env\n",
        "from easyrl.utils.common import load_from_json\n",
        "from easyrl.utils.torch_util import freeze_model\n",
        "from easyrl.utils.torch_util import move_to\n",
        "from easyrl.utils.torch_util import torch_float\n",
        "from easyrl.utils.torch_util import torch_to_np\n",
        "from base64 import b64encode\n",
        "from IPython import display as ipythondisplay\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-jPEdR40oJx"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7rlsrslkGDK"
      },
      "outputs": [],
      "source": [
        "def play_video(video_dir, video_file=None, video_id=None):\n",
        "    if video_file is None:\n",
        "        video_dir = Path(video_dir)\n",
        "        video_files = list(video_dir.glob(f'**/render_video.mp4'))\n",
        "        if video_id is not None:\n",
        "            video_files = [x for x in video_files if f'{video_id:06d}' in x.as_posix()]\n",
        "        video_files.sort()\n",
        "        video_file = video_files[-1]\n",
        "    else:\n",
        "        video_file = Path(video_file)\n",
        "    compressed_file = video_file.parent.joinpath('comp.mp4')\n",
        "    os.system(f\"ffmpeg -i {video_file} -filter:v 'setpts=2.0*PTS' -vcodec libx264 {compressed_file.as_posix()}\")\n",
        "    mp4 = open(compressed_file.as_posix(),'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    ipythondisplay.display(HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))\n",
        "\n",
        "# read tf log file\n",
        "def read_tf_log(log_dir):\n",
        "    log_dir = Path(log_dir)\n",
        "    log_files = list(log_dir.glob(f'**/events.*'))\n",
        "    if len(log_files) < 1:\n",
        "        return None\n",
        "    log_file = log_files[0]\n",
        "    event_acc = EventAccumulator(log_file.as_posix())\n",
        "    event_acc.Reload()\n",
        "    tags = event_acc.Tags()\n",
        "    try:\n",
        "        scalar_success = event_acc.Scalars('train/episode_success')\n",
        "        success_rate = [x.value for x in scalar_success]\n",
        "        steps = [x.step for x in scalar_success]\n",
        "        scalar_return = event_acc.Scalars('train/episode_return/mean')\n",
        "        returns = [x.value for x in scalar_return]\n",
        "    except:\n",
        "        return None\n",
        "    return steps, returns, success_rate\n",
        "\n",
        "\n",
        "def plot_curves(data_dict, title):\n",
        "    # {label: [x, y]}\n",
        "    fig, ax = plt.subplots(figsize=(4, 3))\n",
        "    labels = data_dict.keys()\n",
        "    for label, data in data_dict.items():\n",
        "        x = data[0]\n",
        "        y = data[1]\n",
        "        ax.plot(x, y, label=label)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "# set random seed\n",
        "seed = 0\n",
        "set_random_seed(seed=seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzsXGdpQ0Lno"
      },
      "source": [
        "# Environment (Pusher)\n",
        "\n",
        "In this assignment, we will use the Pusher environment that we used in HW3. We modified the environment so that the goal locations are randomly sampled within a small region. Below, we specify the goal bounds using the variable self._goal_bounds. For comparing performances, we use the success rate and returns. Success rate refers to the proportion of episodes in which the object is pushed to the correct goal location (within the distance threshold of the goal). Return refers to the sum of reward accumulated throughout the trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjjLgXclGC1k"
      },
      "outputs": [],
      "source": [
        "class URRobotPusherGym(gym.Env):\n",
        "    def __init__(self,\n",
        "                 action_repeat=10,\n",
        "                 gui=False,\n",
        "                 max_episode_length=30,\n",
        "                 dist_threshold=0.05):\n",
        "        self._action_repeat = action_repeat\n",
        "        self._max_episode_length = max_episode_length\n",
        "        self._dist_threshold = dist_threshold\n",
        "\n",
        "        self._xy_bounds = np.array([[0.23, 0.78],  # [xmin, xmax]\n",
        "                                    [-0.35, 0.3]])  # [ymin, ymax]\n",
        "        self._goal_bounds = np.array([[0.3, 0.65],  # [xmin, xmax]\n",
        "                                      [0.0, 0.25]])  # [ymin, ymax]\n",
        "        self.robot = Robot('ur5e_stick',\n",
        "                           pb_cfg={'gui': gui,\n",
        "                                   'realtime': False,\n",
        "                                   'opengl_render': torch.cuda.is_available()})\n",
        "        self._arm_reset_pos = np.array([-0.38337763,\n",
        "                                        -2.02650575,\n",
        "                                        -2.01989619,\n",
        "                                        -0.64477803,\n",
        "                                        1.571439041,\n",
        "                                        -0.38331266])\n",
        "        self._table_id = self.robot.pb_client.load_urdf('table/table.urdf',\n",
        "                                                        [.5, 0, 0.4],\n",
        "                                                        euler2quat([0, 0, np.pi / 2]),\n",
        "                                                        scaling=0.9)\n",
        "\n",
        "        # create a ball at the start location (for visualization purpose)\n",
        "        self._start_pos = np.array([0.45, -0.32, 1.0])\n",
        "        self._start_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                             base_pos=self._start_pos,\n",
        "                                                             rgba=[1, 1, 0, 0.8])\n",
        "\n",
        "        # create a ball at the goal location\n",
        "        self._goal_pos = np.array([0.5, 0.2, 1.0])\n",
        "        self._goal_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                            base_pos=self._goal_pos,\n",
        "                                                            rgba=[1, 0, 0, 0.8])\n",
        "\n",
        "        # disable the collision checking between the robot and the ball at the goal location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._goal_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "        # disable the collision checking between the robot and the ball at the start location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._start_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "\n",
        "        self._box_pos = np.array([0.45, -0.1, 0.996])\n",
        "        self._box_id = self.robot.pb_client.load_geom('cylinder', size=[0.05, 0.05], mass=1.,\n",
        "                                                      base_pos=self._box_pos,\n",
        "                                                      rgba=[1., 0.6, 0.6, 1])\n",
        "\n",
        "        self.robot.pb_client.changeDynamics(self._box_id, -1, lateralFriction=0.9)\n",
        "\n",
        "        self.robot.pb_client.setCollisionFilterPair(self._box_id,\n",
        "                                                    self._start_urdf_id,\n",
        "                                                    -1,\n",
        "                                                    -1,\n",
        "                                                    enableCollision=0)\n",
        "        self.robot.pb_client.setCollisionFilterPair(self._box_id,\n",
        "                                                    self._goal_urdf_id,\n",
        "                                                    -1,\n",
        "                                                    -1,\n",
        "                                                    enableCollision=0)\n",
        "\n",
        "        self._action_bound = 1.0\n",
        "        self._ee_pos_scale = 0.04\n",
        "        self._action_high = np.array([self._action_bound] * 2)\n",
        "        self.action_space = spaces.Box(low=-self._action_high,\n",
        "                                       high=self._action_high,\n",
        "                                       dtype=np.float32)\n",
        "        state_low = np.full(len(self._get_obs()), -float('inf'))\n",
        "        state_high = np.full(len(self._get_obs()), float('inf'))\n",
        "        self.observation_space = spaces.Box(state_low,\n",
        "                                            state_high,\n",
        "                                            dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.robot.arm.set_jpos(self._arm_reset_pos, ignore_physics=True)\n",
        "        self.robot.pb_client.reset_body(self._box_id, base_pos=self._box_pos)\n",
        "\n",
        "        starts = self._goal_bounds[:, 0]\n",
        "        ends = self._goal_bounds[:, 1]\n",
        "        width = ends - starts\n",
        "        # different from HW3, we are setting the goal to a random location\n",
        "        ran = np.random.random(2)\n",
        "        goal_pos = starts + width * ran\n",
        "        goal_pos = np.append(goal_pos, 1)\n",
        "        self._goal_pos = goal_pos\n",
        "        self.robot.pb_client.reset_body(self._goal_urdf_id, base_pos=self._goal_pos)\n",
        "\n",
        "        self._t = 0\n",
        "        self._ref_ee_pos = self.robot.arm.get_ee_pose()[0]\n",
        "        self._ref_ee_ori = self.robot.arm.get_ee_pose()[1]\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        previous_state = self._get_obs()\n",
        "        collision = self._apply_action(action)\n",
        "        self._t += 1\n",
        "        state = self._get_obs()\n",
        "\n",
        "        reward, info = self._get_reward(state=state, action=action, previous_state=previous_state)\n",
        "        done = self._t >= self._max_episode_length or info['success']\n",
        "        info['collision'] = collision\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def _get_reward(self, state, action, previous_state):\n",
        "        object_pos = state[2:4]\n",
        "        dist_to_goal = np.linalg.norm(object_pos - self._goal_pos[:2])\n",
        "        success = dist_to_goal < self._dist_threshold\n",
        "        gripper_pos = state[:2]\n",
        "        prev_object_pos = previous_state[2:4]\n",
        "        prev_dist_to_goal = np.linalg.norm(prev_object_pos - self._goal_pos[:2])\n",
        "\n",
        "        gripper_obj_dist = np.linalg.norm(gripper_pos - object_pos)\n",
        "        reach_reward = -gripper_obj_dist\n",
        "        touch_reward = int(gripper_obj_dist < 0.08) * 0.03 if dist_to_goal < prev_dist_to_goal else 0\n",
        "\n",
        "        push_reward = np.exp(-dist_to_goal * 8) * 1. if touch_reward > 0 else 0\n",
        "        if success:\n",
        "            push_reward += 10\n",
        "        reward = touch_reward + push_reward + reach_reward\n",
        "        info = dict(success=success)\n",
        "        return reward, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        gripper_pos = self.robot.arm.get_ee_pose()[0][:2]\n",
        "        object_pos, object_quat = self.robot.pb_client.get_body_state(self._box_id)[:2]\n",
        "        state = np.concatenate([gripper_pos, object_pos[:2], self._goal_pos[:2]])\n",
        "        return state\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        if not isinstance(action, np.ndarray):\n",
        "            action = np.array(action).flatten()\n",
        "        if action.size != 2:\n",
        "            raise ValueError('Action should be [d_x, d_y].')\n",
        "        # we set dz=0\n",
        "        action = np.append(action, 0)\n",
        "        pos, quat, rot_mat, euler = self.robot.arm.get_ee_pose()\n",
        "        pos += action[:3] * self._ee_pos_scale\n",
        "        pos[2] = self._ref_ee_pos[2]\n",
        "        # if the new position is out of the bounds, then we don't apply the action\n",
        "        if not np.logical_and(np.all(pos[:2] >= self._xy_bounds[:, 0]),\n",
        "                              np.all(pos[:2] <= self._xy_bounds[:, 1])):\n",
        "            return False\n",
        "\n",
        "        # move the end-effector to the new position\n",
        "        jnt_pos = self.robot.arm.compute_ik(pos, ori=self._ref_ee_ori)\n",
        "        for step in range(self._action_repeat):\n",
        "            self.robot.arm.set_jpos(jnt_pos)\n",
        "            self.robot.pb_client.stepSimulation()\n",
        "\n",
        "        return False\n",
        "\n",
        "    def render(self, mode='human', **kwargs):\n",
        "        robot_base = self.robot.arm.robot_base_pos\n",
        "        self.robot.cam.setup_camera(focus_pt=robot_base,\n",
        "                                    dist=2,\n",
        "                                    yaw=85,\n",
        "                                    pitch=-20,\n",
        "                                    roll=0)\n",
        "        rgb, _ = self.robot.cam.get_images(get_rgb=True,\n",
        "                                           get_depth=False)\n",
        "        return rgb\n",
        "\n",
        "\n",
        "module_name = __name__\n",
        "\n",
        "env_name = 'URPusher-v1'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{module_name}:URRobotPusherGym',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qShYr_NlkKEl"
      },
      "source": [
        "# Learning from Demonstrations\n",
        "\n",
        "To generate a dataset of demonstrations to learn from, we've provided you a pre-trained expert model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQxMB7CslEY8"
      },
      "outputs": [],
      "source": [
        "# Download the expert model\n",
        "!wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1vlOBeo3caEbl17JEEyxRWbCnFZ--dyXH' -O pusher_expert_model.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYdWirh62l56"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FySbQl8brSF8"
      },
      "outputs": [],
      "source": [
        "def create_actor(env):\n",
        "    ob_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    actor_body = MLP(input_size=ob_dim,\n",
        "                    hidden_sizes=[64],\n",
        "                    output_size=64,\n",
        "                    hidden_act=nn.Tanh,\n",
        "                    output_act=nn.Tanh)\n",
        "    actor = DiagGaussianPolicy(actor_body,\n",
        "                               in_features=64,\n",
        "                               action_dim=action_dim)\n",
        "    return actor\n",
        "\n",
        "def load_expert_agent(env, device, expert_model_path='pusher_expert_model.pt'):\n",
        "    expert_actor = create_actor(env=env)\n",
        "    expert_agent = BasicAgent(actor=expert_actor)\n",
        "    print(f'Loading expert model from: {expert_model_path}.')\n",
        "    ckpt_data = torch.load(expert_model_path, map_location=torch.device(f'{device}'))\n",
        "    load_state_dict(expert_agent.actor,\n",
        "                    ckpt_data['actor_state_dict'])\n",
        "    freeze_model(expert_agent.actor)\n",
        "    return expert_agent\n",
        "\n",
        "def generate_demonstration_data(expert_agent, env, num_trials):\n",
        "    return run_inference(expert_agent, env, num_trials, return_on_done=True)\n",
        "\n",
        "def run_inference(agent, env, num_trials, return_on_done=False, sample=True, disable_tqdm=False, render=False):\n",
        "    runner = EpisodicRunner(agent=agent, env=env)\n",
        "    trajs = []\n",
        "    for trial_id in tqdm(range(num_trials), desc='Run', disable=disable_tqdm):\n",
        "        env.reset()\n",
        "        traj = runner(time_steps=cfg.alg.episode_steps,\n",
        "                      sample=sample,\n",
        "                      return_on_done=return_on_done,\n",
        "                      evaluation=True,\n",
        "                      render_image=render)\n",
        "        trajs.append(traj)\n",
        "    return trajs\n",
        "\n",
        "def eval_agent(agent, env, num_trials, disable_tqdm=False, render=False):\n",
        "    trajs = run_inference(agent, env, num_trials, return_on_done=True, \n",
        "                          disable_tqdm=disable_tqdm, render=render)\n",
        "    tsps = []\n",
        "    successes = []\n",
        "    rets = []\n",
        "    for traj in trajs:\n",
        "        tsps = traj.steps_til_done.copy().tolist()\n",
        "        rewards = traj.raw_rewards\n",
        "        infos = traj.infos\n",
        "        for ej in range(rewards.shape[1]):\n",
        "            ret = np.sum(rewards[:tsps[ej], ej])\n",
        "            rets.append(ret)\n",
        "            successes.append(infos[tsps[ej] - 1][ej]['success'])\n",
        "        if render:\n",
        "            save_traj(traj, 'tmp')\n",
        "    ret_mean = np.mean(rets)\n",
        "    ret_std = np.std(rets)\n",
        "    success_rate = np.mean(successes)\n",
        "    return success_rate, ret_mean, ret_std, rets, successes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVQB5rzR75r5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BasicAgent:\n",
        "    actor: nn.Module\n",
        "\n",
        "    def __post_init__(self):\n",
        "        move_to([self.actor],\n",
        "                device=cfg.alg.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_action(self, ob, sample=True, *args, **kwargs):\n",
        "        t_ob = torch_float(ob, device=cfg.alg.device)\n",
        "        # the policy returns a multi-variate gaussian distribution\n",
        "        act_dist, _ = self.actor(t_ob)\n",
        "        # sample from the distribution\n",
        "        action = action_from_dist(act_dist,\n",
        "                                  sample=sample)\n",
        "        # get the log-probability of the sampled actions\n",
        "        log_prob = action_log_prob(action, act_dist)\n",
        "        # get the entropy of the action distribution\n",
        "        entropy = action_entropy(act_dist, log_prob)\n",
        "        action_info = dict(\n",
        "            log_prob=torch_to_np(log_prob),\n",
        "            entropy=torch_to_np(entropy),\n",
        "        )\n",
        "        return torch_to_np(action), action_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMppa3Af84Jt"
      },
      "outputs": [],
      "source": [
        "def set_configs(exp_name='bc'):\n",
        "    set_config('ppo')\n",
        "    cfg.alg.seed = seed\n",
        "    cfg.alg.num_envs = 1\n",
        "    cfg.alg.episode_steps = 150\n",
        "    cfg.alg.max_steps = 600000\n",
        "    cfg.alg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    cfg.alg.env_name = 'URPusher-v1'\n",
        "    cfg.alg.save_dir = Path.cwd().absolute().joinpath('data').as_posix()\n",
        "    cfg.alg.save_dir += f'/{exp_name}'\n",
        "    setattr(cfg.alg, 'diff_cfg', dict(save_dir=cfg.alg.save_dir))\n",
        "\n",
        "    print(f'====================================')\n",
        "    print(f'      Device:{cfg.alg.device}')\n",
        "    print(f'====================================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LTmNxX-176i"
      },
      "source": [
        "## Generating demonstrations\n",
        "\n",
        "Now that you've downloaded the expert policy model, let's load the expert agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIepfAR0pzuy"
      },
      "outputs": [],
      "source": [
        "# load the expert agent\n",
        "set_configs()\n",
        "env = make_vec_env(cfg.alg.env_name,\n",
        "                   cfg.alg.num_envs,\n",
        "                   seed=cfg.alg.seed)\n",
        "expert_agent = load_expert_agent(env, device=cfg.alg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9STNFnFyJJi"
      },
      "source": [
        "Let's check how good the expert policy is and visualize its performance using `eval_agent` which takes an agent as an input and evaluates it in a specified environment. This function will return to you the succes rate over all trajectories, the mean and standard deviation of total returns, the total returns of each trajectory, and the success of each trajectory. Here, the total "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOJmhwpmyJJj"
      },
      "outputs": [],
      "source": [
        "success_rate, ret_mean, ret_std, rets, successes = eval_agent(expert_agent, env, 500)\n",
        "# you might see some variance in the success rate here, \n",
        "# if you rollout the policies more times, the success rate will be more stable\n",
        "print(f'Expert policy success rate:{success_rate}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_j_Nd4FyxUM"
      },
      "outputs": [],
      "source": [
        "# if you set `render=True`, it will save each evaluation trajectory in `tmp`\n",
        "# and you can use `play_video` to visually check the trajectory\n",
        "success_rate, ret_mean, ret_std, rets, successes = eval_agent(expert_agent, env, 5, render=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzdpunm85f9G"
      },
      "outputs": [],
      "source": [
        "play_video('tmp', video_id=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43jIXtG82l5-"
      },
      "source": [
        "Finally, let's generate a dataset of 50 expert demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StpJmhoHrfSl"
      },
      "outputs": [],
      "source": [
        "expert_trajs = generate_demonstration_data(expert_agent=expert_agent,\n",
        "                                           env=env,\n",
        "                                           num_trials=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oc16tBF2qbS"
      },
      "source": [
        "## Behavior Cloning (BC)\n",
        "\n",
        "Now that we have expert demonstrations, let's use supervised learning (behavior cloning) to clone expert behavior. You will implement the behavior cloning loss to optimize the current policy by comparing the distributions of the expert actions and the current policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llNeMbYH1vB7"
      },
      "outputs": [],
      "source": [
        "class TrajDataset(Dataset):\n",
        "    def __init__(self, trajs):\n",
        "        states = []\n",
        "        actions = []\n",
        "        for traj in trajs:\n",
        "            states.append(traj.obs)\n",
        "            actions.append(traj.actions)\n",
        "        self.states = np.concatenate(states, axis=0)\n",
        "        self.actions = np.concatenate(actions, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.states.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = dict()\n",
        "        sample['state'] = self.states[idx]\n",
        "        sample['action'] = self.actions[idx]\n",
        "        return sample\n",
        "    \n",
        "    def add_traj(self, traj=None, states=None, actions=None):\n",
        "        if traj is not None:\n",
        "            self.states = np.concatenate((self.states, traj.obs), axis=0)\n",
        "            self.actions = np.concatenate((self.actions, traj.actions), axis=0)\n",
        "        else:\n",
        "            self.states = np.concatenate((self.states, states), axis=0)\n",
        "            self.actions = np.concatenate((self.actions, actions), axis=0)            \n",
        "\n",
        "def train_bc_agent(agent, trajs, max_epochs=5000, batch_size=256, lr=0.0005, disable_tqdm=True):\n",
        "    dataset = TrajDataset(trajs)\n",
        "    dataloader = DataLoader(dataset, \n",
        "                            batch_size=batch_size, \n",
        "                            shuffle=True)\n",
        "    optimizer = optim.Adam(agent.actor.parameters(),\n",
        "                           lr=lr)\n",
        "    pbar = tqdm(range(max_epochs), desc='Epoch', disable=disable_tqdm)\n",
        "    logs = dict(loss=[], epoch=[])\n",
        "    for iter in pbar:\n",
        "        avg_loss = []\n",
        "        for batch_idx, sample in enumerate(dataloader):\n",
        "            states = sample['state'].float().to(cfg.alg.device)\n",
        "            expert_actions = sample['action'].float().to(cfg.alg.device)\n",
        "            optimizer.zero_grad()\n",
        "            act_dist, _ = agent.actor(states)\n",
        "            #### TODO: compute the loss in a variable named as 'loss'\n",
        "            #### using the act_dist and expert_actions\n",
        "\n",
        "            \n",
        "            ####\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "            avg_loss.append(loss.item())\n",
        "        logs['loss'].append(np.mean(avg_loss))\n",
        "        logs['epoch'].append(iter)\n",
        "    return agent, logs, len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iissIMPP3em1"
      },
      "source": [
        "**Q1 (30 pts)**: In the real-world collecting expert demonstrations can be expensive. It would be ideal if we can learn a good policy with only a few demonstrations. To gain an understanding of how much data is necessary, lets measure the performance of the learned policy with varying amounts of expert data. Fill in the missing code and train a BC policy using `train_bc_agent` (the default hyperparameters should be sufficient) with $1, 3, 15, 25, 50$ expert trajectories (note that we've already generated 50 trajectories above so you can just index into these!). You should store each of the trained agents (as well as the mean and std of return) for future comparison against in future parts of this assignment. \n",
        "\n",
        "Once you have trained the BC policy, evaluate your policy on $200$ episodes and plot the average success rate (returned by 'eval_agent') as a function of the number of expert transitions used in training. Note that not all trajectories have the same length. We can think of the number of expert transitions as the number of supervised learning examples or in other words, dataset size (which is returned by `train_bc_agent`). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r82rPZFqaJfE"
      },
      "outputs": [],
      "source": [
        "#### TODO: run BC with 1, 3, 15, 25, 50 demonstrations respectively and plot success\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rggQT1s8yJJm"
      },
      "source": [
        "## DAgger\n",
        "\n",
        "As we have learned in the class, behavior cloning suffers from co-variate shift. One way to mitigate this issue is using DAgger. The key idea of DAgger is as follows: \n",
        "1. Train a behavior cloned policy $\\pi_{\\theta_k}$ by using an expert dataset $D$\n",
        "2. Rollout the current policy $\\pi_{\\theta_k}$ and get a trajectory $(s_0, a_0, s_1, a_1, ...)$.\n",
        "3. Query the expert again and get the expert actions $(a^*_0, a^*_1, ...)$. We can add these extra expert demonstration data $D_k=(s_0, a^*_0, s_1, a^*_1, ...)$ to $D$: $D=D\\cup D_k$. \n",
        "4. Optimize the current policy again with the aggregated dataset $D$.\n",
        "5. Repeat step 2 to 4.\n",
        "\n",
        "In the following section, you will implement the loss and policy optimization functions. We have already provided to you steps 1, 2, and 3. You will implement the behavior cloning loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjKKIPqxyJJm"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DaggerAgent:\n",
        "    actor: nn.Module\n",
        "    expert_actor: nn.Module\n",
        "    lr: float\n",
        "\n",
        "    def __post_init__(self):\n",
        "        move_to([self.actor, self.expert_actor],\n",
        "                device=cfg.alg.device)\n",
        "        freeze_model(self.expert_actor)\n",
        "        self.optimizer = optim.Adam(self.actor.parameters(),\n",
        "                                    lr=self.lr)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_action(self, ob, sample=True, *args, **kwargs):\n",
        "        t_ob = torch_float(ob, device=cfg.alg.device)\n",
        "        # the policy returns a multi-variate gaussian distribution\n",
        "        act_dist, _ = self.actor(t_ob)\n",
        "        # sample from the distribution\n",
        "        action = action_from_dist(act_dist,\n",
        "                                  sample=sample)\n",
        "        # get the log-probability of the sampled actions\n",
        "        log_prob = action_log_prob(action, act_dist)\n",
        "        # get the entropy of the action distribution\n",
        "        entropy = action_entropy(act_dist, log_prob)\n",
        "        action_info = dict(\n",
        "            log_prob=torch_to_np(log_prob),\n",
        "            entropy=torch_to_np(entropy),\n",
        "        )\n",
        "        # get the expert action from the expert policy\n",
        "        exp_act_dist, _ = self.expert_actor(t_ob)\n",
        "        action_info['exp_act'] = exp_act_dist.mean\n",
        "        return torch_to_np(action), action_info\n",
        "\n",
        "    def optimize(self, data, **kwargs):\n",
        "        for key, val in data.items():\n",
        "            data[key] = torch_float(val, device=cfg.alg.device)\n",
        "        ob = data['state']\n",
        "        exp_act = data['action']\n",
        "        act_dist, _ = self.actor(x=ob)\n",
        "        #### TODO: compute the loss in a variable named as 'loss'\n",
        "        #### using the act_dist and exp_act\n",
        "        \n",
        "        ####\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        grad_norm = clip_grad(self.actor.parameters(),\n",
        "                              cfg.alg.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        optim_info = dict(\n",
        "            loss=loss.item(),\n",
        "            grad_norm=grad_norm,\n",
        "        )\n",
        "        return optim_info\n",
        "    \n",
        "    def save_model(self, is_best=False, step=None):\n",
        "        data_to_save = {\n",
        "            'actor_state_dict': self.actor.state_dict()\n",
        "        }\n",
        "        save_model(data_to_save, cfg.alg, is_best=is_best, step=step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi2w-7rMyJJm"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DaggerEngine:\n",
        "    agent: Any\n",
        "    runner: Any\n",
        "    env: Any\n",
        "    trajs: Any\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.dataset = TrajDataset(self.trajs)\n",
        "        \n",
        "    def train(self):\n",
        "        success_rates = []\n",
        "        dataset_sizes = []\n",
        "        self.cur_step = 0\n",
        "        for iter_t in count():\n",
        "            if iter_t % cfg.alg.eval_interval == 0:\n",
        "                success_rate, ret_mean, ret_std, rets, successes = eval_agent(self.agent, \n",
        "                                                                              self.env, \n",
        "                                                                              200,\n",
        "                                                                              disable_tqdm=True)\n",
        "                success_rates.append(success_rate)\n",
        "                dataset_sizes.append(len(self.dataset))\n",
        "            # rollout the current policy and get a trajectory\n",
        "            traj = self.runner(sample=True, get_last_val=False, time_steps=cfg.alg.episode_steps)\n",
        "            # optimize the policy\n",
        "            self.train_once(traj)\n",
        "            if self.cur_step > cfg.alg.max_steps:\n",
        "                break\n",
        "        return dataset_sizes, success_rates\n",
        "\n",
        "    def train_once(self, traj):\n",
        "        self.cur_step += traj.total_steps\n",
        "\n",
        "        action_infos = traj.action_infos\n",
        "        exp_act = torch.stack([ainfo['exp_act'] for ainfo in action_infos])\n",
        "\n",
        "        self.dataset.add_traj(states=traj.obs,\n",
        "                              actions=exp_act.cpu())\n",
        "        rollout_dataloader = DataLoader(self.dataset,\n",
        "                                        batch_size=cfg.alg.batch_size,\n",
        "                                        shuffle=True,\n",
        "                                       )\n",
        "        optim_infos = []\n",
        "        for oe in range(cfg.alg.opt_epochs):\n",
        "            for batch_ndx, batch_data in enumerate(rollout_dataloader):\n",
        "                optim_info = self.agent.optimize(batch_data)\n",
        "                optim_infos.append(optim_info)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmR9pws5yJJm"
      },
      "outputs": [],
      "source": [
        "def train_dagger(expert_actor, trajs, actor=None):\n",
        "    expert_actor = deepcopy(expert_actor)\n",
        "    actor = deepcopy(actor)\n",
        "    set_configs('dagger')\n",
        "    cfg.alg.episode_steps = 30\n",
        "    cfg.alg.max_steps = 1200\n",
        "    cfg.alg.eval_interval = 1\n",
        "    cfg.alg.log_interval = 1\n",
        "    cfg.alg.batch_size = 256\n",
        "    cfg.alg.opt_epochs = 500\n",
        "    set_random_seed(cfg.alg.seed)\n",
        "    env = make_vec_env(cfg.alg.env_name,\n",
        "                       cfg.alg.num_envs,\n",
        "                       seed=cfg.alg.seed)\n",
        "    env.reset()\n",
        "    if actor is None:\n",
        "        actor = create_actor(env=env)\n",
        "    dagger_agent = DaggerAgent(actor=actor, expert_actor=expert_actor, lr=0.001)\n",
        "    runner = EpisodicRunner(agent=dagger_agent, env=env)\n",
        "    engine = DaggerEngine(agent=dagger_agent,\n",
        "                          env=env,\n",
        "                          runner=runner,\n",
        "                          trajs=trajs)\n",
        "    dataset_sizes, success_rates = engine.train()\n",
        "    return dagger_agent, dataset_sizes, success_rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiCsY4bfLuK4"
      },
      "source": [
        "**Q2 (30 pts)**: Complete the missing code for `DaggerAgent` (define the loss function and optimize the policy). Train a DAgger agent using `train_dagger` with the behaviorally cloned agent that was trained on just one demonstration as the initial policy, $\\pi_{\\theta_k}$ (the same demonstration as the initial dataset $D$), and the expert that will provide expert actions as our pre-trained expert model.\n",
        "\n",
        "Similar to Q1, plot the success rate curves for both DAgger and BC as a function of the number of expert transitions (e.g., dataset size) available at training. This would be your x-axis. Your DAgger and BC curves should reach similarly high performance with enough data while DAgger will reach a high success rate earlier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUu6sO9VyJJn"
      },
      "outputs": [],
      "source": [
        "#### TODO: train DAgger agent \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### TODO: plot the success rate curves for BC and DAgger together\n"
      ],
      "metadata": {
        "id": "J7UZhTSoEDxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q (10 pts):** Why does DAgger achieve better performance than BC when they both use datasets of the same size?\n",
        "\n",
        "**A:**"
      ],
      "metadata": {
        "id": "_oc_wgFMuCWC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USXTaSC-yJJo"
      },
      "source": [
        "# RL Finetune\n",
        "\n",
        "In DAgger, an expert is required to query the optimal action at every step during training time. However, an expert may not always be available. In the context of autonomous vehicles, for behavior cloning an expert driver is only required to collect the initial dataset of expert demonstrations while for DAgger we would need an expert to see everything the vehicle does and relabel the data in a continuous cycle during all of training. Clearly this approach does not scale, especially if obtaining these expert labels is costly.\n",
        "\n",
        "In such cases where an expert is unavailable during the training process, if we have access to the reward function in addition to a few demonstrations, then we can combine RL and behavior cloning to improve performance. \n",
        "\n",
        "In the following section, we will use PPO to finetune the behavior-cloned policy. We have provided you with a `train_ppo` function that you are free to modify. Note that typically we use multi-process to roll out many agents in parallel in PPO. It typically leads to faster and more stable learning. One way to get around parallelization is to have one policy update step after collecting many rollouts with the same policy. Here, our environment terminates after $30$ steps and gets reset, but `episode_steps` is set to 900, so we will have $30$ agents' rollout experience for every single step of policy optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG9_a5WtyJJo"
      },
      "outputs": [],
      "source": [
        "def train_ppo(actor=None, save_dir=None, max_steps=1000000):\n",
        "    set_config('ppo')\n",
        "    cfg.alg.num_envs = 1\n",
        "    cfg.alg.episode_steps = 900\n",
        "    cfg.alg.max_steps = max_steps\n",
        "    cfg.alg.deque_size = 20\n",
        "    cfg.alg.eval_interval = 10\n",
        "    cfg.alg.log_interval = 1\n",
        "    cfg.alg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    cfg.alg.env_name = 'URPusher-v1'\n",
        "    cfg.alg.save_dir = Path.cwd().absolute().joinpath('data').as_posix()\n",
        "    cfg.alg.save_dir += '/rl_finetune' if save_dir is None else f'/{save_dir}'\n",
        "    setattr(cfg.alg, 'diff_cfg', dict(save_dir=cfg.alg.save_dir))\n",
        "\n",
        "    print(f'====================================')\n",
        "    print(f'      Device:{cfg.alg.device}')\n",
        "    print(f'      Total number of steps:{cfg.alg.max_steps}')\n",
        "    print(f'====================================')\n",
        "\n",
        "    set_random_seed(cfg.alg.seed)\n",
        "    env = make_vec_env(cfg.alg.env_name,\n",
        "                       cfg.alg.num_envs,\n",
        "                       seed=cfg.alg.seed)\n",
        "    env.reset()\n",
        "    ob_size = env.observation_space.shape[0]\n",
        "\n",
        "    if actor is None:\n",
        "        actor = create_actor(env=env)\n",
        "    actor = deepcopy(actor)\n",
        "\n",
        "    critic_body = MLP(input_size=ob_size,\n",
        "                     hidden_sizes=[64],\n",
        "                     output_size=64,\n",
        "                     hidden_act=nn.Tanh,\n",
        "                     output_act=nn.Tanh)\n",
        "\n",
        "    critic = ValueNet(critic_body, in_features=64)\n",
        "    agent = PPOAgent(actor=actor, critic=critic, env=env)\n",
        "    runner = EpisodicRunner(agent=agent, env=env)\n",
        "    engine = PPOEngine(agent=agent,\n",
        "                       runner=runner)\n",
        "    engine.train()\n",
        "    return agent, engine, cfg.alg.save_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLbAZ37RyJJo"
      },
      "source": [
        "**Q3 (20 pts)**: Compare the learning curves (both the return curve and the success rate curve) of the following two cases. You may find `read_tf_log` useful. Train for 600K steps a behavior-cloned policy with RL (specifically here, use the BC agent trained with 3 demonstration trajectories from Q1 and use RL-finetuning). You will compare the results of the BC policies with a policy trained from scratch, which we provided to you and you will download.\n",
        "\n",
        "The purpose of this question and the next is to plot for scientific purposes.  In RL, uncertainty is often quantified by the standard deviation across different seeds. In this problem, we haven't provided different seeds, so we will instead measure the uncertainty by the standard deviation across evaluation epsodes.\n",
        "\n",
        "On the return and success rate curves, also plot the average return with its uncertainty (in terms of standard deviation over the evaluation episodes) as well as the average success rate for the BC agent trained on 3 demonstrations without any fine-turning (you should've cached this from Q1). This should just be a horizontal line. The x-axis is the number of steps and the y-axis is the average returns.\n",
        "\n",
        "For the sake of consistency, please follow the following conventions. Use red for the PPO agent (trained from scratch). Use green for the BC agent trained only on 3 demonstrations. Use blue for the BC agent trained with 3 demonstrations and then finetuned. Please add a legend to your plot to clearly identify each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbSvSvCWyJJo"
      },
      "outputs": [],
      "source": [
        "# Download the results of the model trained with PPO from scratch\n",
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1o8V3SMT0Mg8sa0X7S_YtQz5KWE8a116i' -O rl_scratch_results.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"rl_scratch_results.csv\")\n",
        "scratch_steps, scratch_returns, scratch_success_rate = df[\"steps\"], df[\"returns\"], df[\"success_rate\"]"
      ],
      "metadata": {
        "id": "kr2ijKbK7kkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TODO: finetune the behavior cloned policy (trained with 3 demonstration trajectories\n"
      ],
      "metadata": {
        "id": "rfK3txf-0Kei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TODO: plot the average returns with uncertainties (in terms of standard deviation)\n"
      ],
      "metadata": {
        "id": "WkKbOagU0T03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q (20 pts):** Do you see any difference in performance at the end of training? Do you notice changes in the overall performance? What about in comparison to the BC agent without fine-tuning? If so, describe them in detail.\n",
        "\n",
        "**A:**"
      ],
      "metadata": {
        "id": "Tg1dXaEI3_zO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XKefZnUkhn9"
      },
      "source": [
        "# Suboptimal Demonstrations\n",
        "\n",
        "In many cases, we might not have a good expert model available. Thus, the demonstrations we get from the expert will not be optimal. In the following section, we will use a sub-optimal expert model to generate the demonstration data, use such data to train a BC agent, and see if the policy can be improved by RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_rn4MTlkgwd"
      },
      "outputs": [],
      "source": [
        "# Download the suboptimal expert model\n",
        "!wget --no-check-certificate -r 'https://docs.google.com/uc?export=download&id=1AzWwGkOcZxrx43kNhD-7TPj06bIKRSXX' -O pusher_suboptimal_expert_model.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm6zsswIk-GG"
      },
      "outputs": [],
      "source": [
        "# same as before, let's load a suboptimal expert model\n",
        "set_configs()\n",
        "env = make_vec_env(cfg.alg.env_name,\n",
        "                   cfg.alg.num_envs,\n",
        "                   seed=cfg.alg.seed)\n",
        "sub_expert_agent = load_expert_agent(env, device=cfg.alg.device, expert_model_path='pusher_suboptimal_expert_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46UVq87olOtp"
      },
      "outputs": [],
      "source": [
        "# let's see how good the expert model is\n",
        "success_rate, ret_mean, ret_std, rets, successes = eval_agent(sub_expert_agent, env, 500)\n",
        "print(f'Expert policy success rate:{success_rate}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSKBXNkemyvr"
      },
      "outputs": [],
      "source": [
        "# let's use this suboptimal expert model to generate some demonstrations\n",
        "sub_expert_trajs = generate_demonstration_data(expert_agent=sub_expert_agent,\n",
        "                                               env=env,\n",
        "                                               num_trials=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hkjfWp22l6F"
      },
      "source": [
        "**Q4.1 (10 pts)**: Similar to Q1, train a BC policy using `train_bc_agent` with $1, 3, 15, 25, 50$ suboptimal trajectories. Once you have trained the BC policy (cloned on suboptimal data), evaluate your policy on $200$ episodes and plot the average success rate (returned by 'eval_agent') as a function of the number of expert transitions used in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZDHchoCmfRQ"
      },
      "outputs": [],
      "source": [
        "### TODO run BC with 1, 3, 15, 25, 50 demonstrations respectively and plot success\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M70CtVFNFKwN"
      },
      "source": [
        "**Q4.2 (20 pts)**: Similar to in Q3, run RL-finetune with the behavior-cloned policy trained from 3 suboptimal demonstration trajectories. Plot the return and success rate of the agent. On the same graph, also plot the same average returns for the trained from scratch and RL-finetuned on 3 optimal demonstrations policies. \n",
        "\n",
        "For the sake of consistency, please follow the following conventions. Use red for the PPO agent (trained from scratch). Use green for the BC agent trained only on 3 demonstrations. Use blue for the BC agent trained with 3 demonstrations and then finetuned. Use pink for the BC agent trained with 3 suboptimal demonstrations and then finetuned. Please add a legend to your plot to clearly identify each method.\n",
        "\n",
        "Does RL-finetune help improve the policy performance?\n",
        "\n",
        "**A**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5Zt-D0gtXuv"
      },
      "outputs": [],
      "source": [
        "### TODO finetune bc policy trained on 3 suboptimal demonstrations. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO plot in comparison with from scratch and finetune on optimal demonstrations"
      ],
      "metadata": {
        "id": "gLtpoGwW3cjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKnGJE7JqE2K"
      },
      "source": [
        "# More Advanced Ideas (Bonus, Optional)\n",
        "\n",
        "So far, we have seen how behavior cloning, DAgger, and behavior cloning with RL finetuning work. There are many more ideas one can try to make use of the demonstration data. For example, we can optimize the policy with the RL loss and behavior-cloning loss together as done in this work - [Learning Complex Dexterous Manipulation with\n",
        "Deep Reinforcement Learning and Demonstrations, Equation (6)](https://arxiv.org/pdf/1709.10087.pdf) - discussed in lecture.\n",
        " \n",
        "Try to implement it yourself on the Pusher environment. You are free to use any publicly avaiable RL library. You can also try out a sparse-reward setting for the Pusher environment with expert demonstrations. \n",
        "\n",
        "**Q (20 pts)**: Plot the return as well as the success rate curves. Compare the curves with the curves from the RL-scratch (training with RL from scratch) experiment. Use a similar plotting scheme as the previous questions.\n",
        "\n",
        "**A**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvRqdXaKVtcL"
      },
      "source": [
        "# Survey (bonus points, 10 pts)\n",
        "Please fill out [this anonymous survey](https://forms.gle/ScoTb2DKJEZHMkTMA) and enter the code below to receive credit. Thanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IJcl7M8yJJp"
      },
      "source": [
        "**Code:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20q_lA3F2l6I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}